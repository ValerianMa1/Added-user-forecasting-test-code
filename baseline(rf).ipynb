{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "train_data['ms'] = train_data['common_ts']\n",
    "test_data['ms'] = test_data['common_ts']\n",
    "train_data['common_ts'] = pd.to_datetime(train_data['common_ts'], unit='ms')\n",
    "test_data['common_ts'] = pd.to_datetime(test_data['common_ts'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def udmap_onethot(d):\n",
    "#     v = np.zeros(9)\n",
    "#     if d == 'unknown':\n",
    "#         return v\n",
    "    \n",
    "#     d = eval(d)\n",
    "#     for i in range(1, 10):\n",
    "#         if 'key' + str(i) in d:\n",
    "#             v[i-1] = d['key' + str(i)]\n",
    "            \n",
    "#     return v\n",
    "\n",
    "# train_udmap_df = pd.DataFrame(np.vstack(train_data['udmap'].apply(udmap_onethot)))\n",
    "# test_udmap_df = pd.DataFrame(np.vstack(test_data['udmap'].apply(udmap_onethot)))\n",
    "\n",
    "# train_udmap_df.columns = ['key' + str(i) for i in range(1, 10)]\n",
    "# test_udmap_df.columns = ['key' + str(i) for i in range(1, 10)]\n",
    "# # test_data['udmap_isunknown'] = (test_data['udmap'] == 'unknown').astype(int)\n",
    "# train_udmap_df = pd.concat([train_udmap_df, (train_data['udmap'] == 'unknown').astype(int)], axis=1)\n",
    "# test_udmap_df = pd.concat([test_udmap_df, (test_data['udmap'] == 'unknown').astype(int)], axis=1)\n",
    "# # 标准化数据\n",
    "# scaler = StandardScaler()\n",
    "# train_udmap_df = scaler.fit_transform(train_udmap_df)\n",
    "# test_udmap_df = scaler.fit_transform(test_udmap_df)\n",
    "# # 应用PCA\n",
    "# pca = PCA(n_components=3)\n",
    "# train_udmap_df = pca.fit_transform(train_udmap_df)\n",
    "# test_udmap_df = pca.fit_transform(test_udmap_df)\n",
    "# # 将PCA结果转为DataFrame并与原始DataFrame结合\n",
    "# train_udmap_df = pd.DataFrame(data=train_udmap_df, columns=['keyPC' + str(i) for i in range(1, 4)])\n",
    "# test_udmap_df = pd.DataFrame(data=test_udmap_df, columns=['keyPC' + str(i) for i in range(1, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode, skew, entropy\n",
    "# train_data = pd.concat([train_data, train_udmap_df], axis=1)\n",
    "# test_data = pd.concat([test_data, test_udmap_df], axis=1)\n",
    "\n",
    "# 计算均值和标准差\n",
    "# train_data['key_mean'] = train_udmap_df[['key' + str(i) for i in range(1, 10)]].mean(axis=1)\n",
    "# train_data['key_std'] = train_udmap_df[['key' + str(i) for i in range(1, 10)]].std(axis=1)\n",
    "# test_data['key_mean'] = test_udmap_df[['key' + str(i) for i in range(1, 10)]].mean(axis=1)\n",
    "# test_data['key_std'] = test_udmap_df[['key' + str(i) for i in range(1, 10)]].std(axis=1)\n",
    "\n",
    "# train_data['key_median'] = train_udmap_df[['key' + str(i) for i in range(1, 10)]].median(axis=1)\n",
    "# train_data['key_mode'] = train_udmap_df[['key' + str(i) for i in range(1, 10)]].apply(lambda row: mode(row)[0][0], axis=1)\n",
    "# test_data['key_median'] = test_udmap_df[['key' + str(i) for i in range(1, 10)]].median(axis=1)\n",
    "# test_data['key_mode'] = test_udmap_df[['key' + str(i) for i in range(1, 10)]].apply(lambda row: mode(row)[0][0], axis=1)\n",
    "# train_data['key_Q1'] = train_udmap_df[['key' + str(i) for i in range(1, 10)]].quantile(0.25, axis=1)\n",
    "# test_data['key_Q1'] = test_udmap_df[['key' + str(i) for i in range(1, 10)]].quantile(0.25, axis=1)\n",
    "# train_data['key_Q3'] = train_udmap_df[['key' + str(i) for i in range(1, 10)]].quantile(0.75, axis=1)\n",
    "# test_data['key_Q3'] = test_udmap_df[['key' + str(i) for i in range(1, 10)]].quantile(0.75, axis=1)\n",
    "# train_data['key_skew'] = train_udmap_df[['key' + str(i) for i in range(1, 10)]].skew(axis=1)\n",
    "# test_data['key_skew'] = test_udmap_df[['key' + str(i) for i in range(1, 10)]].skew(axis=1)\n",
    "# train_data['key_entropy'] = train_udmap_df[['key' + str(i) for i in range(1, 10)]].apply(lambda row: entropy(row.value_counts(normalize=True)), axis=1)\n",
    "# test_data['key_entropy'] = test_udmap_df[['key' + str(i) for i in range(1, 10)]].apply(lambda row: entropy(row.value_counts(normalize=True)), axis=1)\n",
    "\n",
    "# train_data['key_max'] = train_udmap_df[['key' + str(i) for i in range(1, 10)]].max(axis=1)\n",
    "# test_data['key_max'] = test_udmap_df[['key' + str(i) for i in range(1, 10)]].max(axis=1)\n",
    "# train_data['key_min'] = train_udmap_df[['key' + str(i) for i in range(1, 10)]].min(axis=1)\n",
    "# test_data['key_min'] = test_udmap_df[['key' + str(i) for i in range(1, 10)]].min(axis=1)\n",
    "# train_data['key_range'] = train_data['key_max'] - train_data['key_min']\n",
    "# test_data['key_range'] = test_data['key_max'] - test_data['key_min']\n",
    "# # 变异系数 (使用标准差和均值)\n",
    "# train_data['key_CV'] = train_data['key_std'] / train_data['key_mean']\n",
    "# test_data['key_CV'] = test_data['key_std'] / test_data['key_mean']\n",
    "\n",
    "train_data['x_mean'] = train_data[['x' + str(i) for i in range(1, 9)]].mean(axis=1)\n",
    "train_data['x_std'] = train_data[['x' + str(i) for i in range(1, 9)]].std(axis=1)\n",
    "test_data['x_mean'] = test_data[['x' + str(i) for i in range(1, 9)]].mean(axis=1)\n",
    "test_data['x_std'] = test_data[['x' + str(i) for i in range(1, 9)]].std(axis=1)\n",
    "\n",
    "# train_data['x_median'] = train_data[['x' + str(i) for i in range(1, 9)]].median(axis=1)\n",
    "# train_data['x_mode'] = train_data[['x' + str(i) for i in range(1, 9)]].apply(lambda row: mode(row)[0][0], axis=1)\n",
    "# test_data['x_median'] = test_data[['x' + str(i) for i in range(1, 9)]].median(axis=1)\n",
    "# test_data['x_mode'] = test_data[['x' + str(i) for i in range(1, 9)]].apply(lambda row: mode(row)[0][0], axis=1)\n",
    "train_data['x_Q1'] = train_data[['x' + str(i) for i in range(1, 9)]].quantile(0.25, axis=1)\n",
    "test_data['x_Q1'] = test_data[['x' + str(i) for i in range(1, 9)]].quantile(0.25, axis=1)\n",
    "train_data['x_Q3'] = train_data[['x' + str(i) for i in range(1, 9)]].quantile(0.75, axis=1)\n",
    "test_data['x_Q3'] = test_data[['x' + str(i) for i in range(1, 9)]].quantile(0.75, axis=1)\n",
    "train_data['x_skew'] = train_data[['x' + str(i) for i in range(1, 9)]].skew(axis=1)\n",
    "test_data['x_skew'] = test_data[['x' + str(i) for i in range(1, 9)]].skew(axis=1)\n",
    "# train_data['x_entropy'] = train_data[['x' + str(i) for i in range(1, 9)]].apply(lambda row: entropy(row.value_counts(normalize=True)), axis=1)\n",
    "# test_data['x_entropy'] = test_data[['x' + str(i) for i in range(1, 9)]].apply(lambda row: entropy(row.value_counts(normalize=True)), axis=1)\n",
    "# train_data['x_max'] = train_data[['x' + str(i) for i in range(1, 9)]].max(axis=1)\n",
    "# test_data['x_max'] = test_data[['x' + str(i) for i in range(1, 9)]].max(axis=1)\n",
    "# train_data['x_min'] = train_data[['x' + str(i) for i in range(1, 9)]].min(axis=1)\n",
    "# test_data['x_min'] = test_data[['x' + str(i) for i in range(1, 9)]].min(axis=1)\n",
    "# train_data['x_range'] = train_data['x_max'] - train_data['x_min']\n",
    "# test_data['x_range'] = test_data['x_max'] - test_data['x_min']\n",
    "# 变异系数 (使用标准差和均值)\n",
    "train_data['x_CV'] = train_data['x_std'] / train_data['x_mean']\n",
    "test_data['x_CV'] = test_data['x_std'] / test_data['x_mean']\n",
    "# train_data = train_data.drop(['x' + str(i) for i in range(6, 9)], axis=1)\n",
    "# test_data = test_data.drop(['x' + str(i) for i in range(6, 9)], axis=1)\n",
    "# train_data = train_data.drop(['key6', 'key7', 'key8', 'key9'], axis=1)\n",
    "# test_data = test_data.drop(['key6', 'key7', 'key8', 'key9'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 标准化数据\n",
    "# scaler = StandardScaler()\n",
    "# train_x = scaler.fit_transform(train_data[['x' + str(i) for i in range(1, 6)]])\n",
    "# test_x = scaler.fit_transform(test_data[['x' + str(i) for i in range(1, 6)]])\n",
    "# # 应用PCA\n",
    "# n_components = 1\n",
    "# pca = PCA(n_components=n_components)\n",
    "# train_x = pca.fit_transform(train_x)\n",
    "# test_x = pca.fit_transform(test_x)\n",
    "# # 将PCA结果转为DataFrame并与原始DataFrame结合\n",
    "# train_x = pd.DataFrame(data=train_x, columns=['xPC' + str(i) for i in range(1, n_components+1)])\n",
    "# test_x = pd.DataFrame(data=test_x, columns=['xPC' + str(i) for i in range(1, n_components+1)])\n",
    "# # train_data = train_data.drop(['x' + str(i) for i in range(1, 9)], axis=1)\n",
    "# # test_data = test_data.drop(['x' + str(i) for i in range(1, 9)], axis=1)\n",
    "# train_data = pd.concat([train_data, train_x], axis=1)\n",
    "# test_data = pd.concat([test_data, test_x], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['eid_freq'] = train_data['eid'].map(train_data['eid'].value_counts())\n",
    "# test_data['eid_freq'] = test_data['eid'].map(train_data['eid'].value_counts())\n",
    "\n",
    "# train_data['eid_mean'] = train_data['eid'].map(train_data.groupby('eid')['target'].mean())\n",
    "# test_data['eid_mean'] = test_data['eid'].map(train_data.groupby('eid')['target'].mean())\n",
    "# train_data['eid_std'] = train_data['eid'].map(train_data.groupby('eid')['target'].std())\n",
    "# test_data['eid_std'] = test_data['eid'].map(train_data.groupby('eid')['target'].std())\n",
    "# train_data['eid_skew'] = train_data['eid'].map(train_data.groupby('eid')['target'].skew())\n",
    "# test_data['eid_skew'] = test_data['eid'].map(train_data.groupby('eid')['target'].skew())\n",
    "# train_data['eid_Q1'] = train_data['eid'].map(train_data.groupby('eid')['target'].quantile(0.25))\n",
    "# test_data['eid_Q1'] = test_data['eid'].map(train_data.groupby('eid')['target'].quantile(0.25))\n",
    "# train_data['eid_Q3'] = train_data['eid'].map(train_data.groupby('eid')['target'].quantile(0.75))\n",
    "# test_data['eid_Q3'] = test_data['eid'].map(train_data.groupby('eid')['target'].quantile(0.75))\n",
    "# train_data['eid_entropy'] = train_data['eid'].map(train_data.groupby('eid')['target'].apply(lambda row: entropy(row.value_counts(normalize=True))))\n",
    "# test_data['eid_entropy'] = test_data['eid'].map(train_data.groupby('eid')['target'].apply(lambda row: entropy(row.value_counts(normalize=True))))\n",
    "train_data = train_data.drop('eid', axis=1)\n",
    "test_data = test_data.drop('eid', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['common_ts_weekday'] = train_data['common_ts'].dt.weekday\n",
    "# for i in range(6):\n",
    "#     train_data[f'common_isday{i+1}'] = (train_data['common_ts'].dt.weekday == i).astype(int)\n",
    "# train_data['common_isThursday'] = (train_data['common_ts'].dt.weekday == 3).astype(int)\n",
    "train_data['common_ts_day'] = train_data['common_ts'].dt.day\n",
    "train_data['common_ts_hour'] = train_data['common_ts'].dt.hour\n",
    "# train_data['common_ts_minute'] = train_data['common_ts'].dt.minute\n",
    "# train_data['common_ts_second'] = train_data['common_ts'].dt.second\n",
    "test_data['common_ts_weekday'] = test_data['common_ts'].dt.weekday\n",
    "# for i in range(6):\n",
    "#     test_data[f'common_isday{i+1}'] = (test_data['common_ts'].dt.weekday == i).astype(int)\n",
    "# test_data['common_isMonday'] = (test_data['common_ts'].dt.weekday == 0).astype(int)\n",
    "# test_data['common_isThursday'] = (test_data['common_ts'].dt.weekday == 3).astype(int)\n",
    "test_data['common_ts_day'] = test_data['common_ts'].dt.day\n",
    "test_data['common_ts_hour'] = test_data['common_ts'].dt.hour\n",
    "# test_data['common_ts_minute'] = test_data['common_ts'].dt.minute\n",
    "# test_data['common_ts_second'] = test_data['common_ts'].dt.second\n",
    "train_data = train_data.drop(['udmap', 'common_ts', 'uuid'], axis=1)\n",
    "test_data = test_data.drop(['udmap', 'common_ts'], axis=1)\n",
    "# train_data.fillna(0, inplace=True)\n",
    "# test_data.fillna(0, inplace=True)\n",
    "# 将特征列赋值给X，将目标列赋值给y\n",
    "X = train_data.drop(['target'], axis=1)\n",
    "y = train_data['target']\n",
    "# smote = SMOTE()\n",
    "# X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "# # 划分训练集和测试集\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9644722419240441\n",
      "F1 score: 0.8670527204729159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# default_params = {\n",
    "#     'n_estimators': 100,\n",
    "#     'criterion': \"gini\",\n",
    "#     'max_depth': None, \n",
    "#     'min_samples_split': 2,\n",
    "#     'min_samples_leaf': 1,\n",
    "#     'min_weight_fraction_leaf': 0.0,\n",
    "#     'max_features': \"sqrt\",\n",
    "#     'max_leaf_nodes': None, \n",
    "#     'min_impurity_decrease': 0.0,\n",
    "#     'bootstrap': True,\n",
    "#     'oob_score': False,\n",
    "# }\n",
    "\n",
    "# best_params = {\n",
    "#   'bootstrap': True,\n",
    "#   'criterion': 'entropy', \n",
    "#   'max_depth': 21, \n",
    "#   'max_features': 'log2', \n",
    "#   'max_leaf_nodes': 107, \n",
    "#   'min_impurity_decrease': 0.009848041335654867, \n",
    "#   'min_samples_leaf': 4.769754941069463, \n",
    "#   'min_samples_split': 2.6299871207540337, \n",
    "#   'n_estimators': 100,\n",
    "# }\n",
    "# final_params = best_params.copy()\n",
    "# final_params.update(default_params)\n",
    "# print(final_params)\n",
    "clf = RandomForestClassifier(n_estimators=131,\n",
    "                             max_depth=33,\n",
    "                             n_jobs=-1,\n",
    "                             max_features=9,\n",
    "                             min_samples_leaf=1,\n",
    "                             min_samples_split=2,\n",
    "                             criterion = 'entropy'\n",
    "                             )\n",
    "clf.fit(\n",
    "    train_data.drop(['target'], axis=1),\n",
    "    train_data['target']\n",
    ")\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# 计算F1分数\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- eid\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[0;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39muuid\u001b[39m\u001b[39m'\u001b[39m: test_data[\u001b[39m'\u001b[39m\u001b[39muuid\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m----> 3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m: clf\u001b[39m.\u001b[39;49mpredict(test_data\u001b[39m.\u001b[39;49mdrop([\u001b[39m'\u001b[39;49m\u001b[39muuid\u001b[39;49m\u001b[39m'\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m      4\u001b[0m })\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mresultwithentropy.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\asd\\.conda\\envs\\py3810\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:820\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    800\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[39m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[0;32m    822\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    823\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\asd\\.conda\\envs\\py3810\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:862\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    860\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    861\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[0;32m    864\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    865\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\asd\\.conda\\envs\\py3810\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:602\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 602\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    603\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[0;32m    604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\asd\\.conda\\envs\\py3810\\lib\\site-packages\\sklearn\\base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[0;32m    490\u001b[0m ):\n\u001b[0;32m    491\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \n\u001b[0;32m    493\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    551\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\asd\\.conda\\envs\\py3810\\lib\\site-packages\\sklearn\\base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    477\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m     )\n\u001b[1;32m--> 481\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- eid\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'uuid': test_data['uuid'],\n",
    "    'target': clf.predict(test_data.drop(['uuid'], axis=1))\n",
    "}).to_csv('resultwithentropy.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_tree_model(entropy).pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# 保存模型\n",
    "# 保存模型到文件\n",
    "joblib.dump(clf, 'rf_tree_model(entropy).pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- eid\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m clf \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mrf_tree_model(entropy).pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# conf_thres = 0.5\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# 在测试集上进行预测\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39;49mpredict(X_val)\n\u001b[0;32m      8\u001b[0m \u001b[39m# 计算准确率\u001b[39;00m\n\u001b[0;32m      9\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(y_val, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\asd\\.conda\\envs\\py3810\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:820\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    800\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[39m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[0;32m    822\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    823\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\asd\\.conda\\envs\\py3810\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:862\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    860\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    861\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[0;32m    864\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    865\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\asd\\.conda\\envs\\py3810\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:602\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 602\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    603\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[0;32m    604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\asd\\.conda\\envs\\py3810\\lib\\site-packages\\sklearn\\base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[0;32m    490\u001b[0m ):\n\u001b[0;32m    491\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \n\u001b[0;32m    493\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    551\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\asd\\.conda\\envs\\py3810\\lib\\site-packages\\sklearn\\base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    477\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m     )\n\u001b[1;32m--> 481\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- eid\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "clf = joblib.load('rf_tree_model(entropy).pkl')\n",
    "\n",
    "# conf_thres = 0.5\n",
    "# 在测试集上进行预测\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# 计算F1分数\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers =[3,1.0]\n",
    "\n",
    "# for number in numbers:\n",
    "#     clf = RandomForestClassifier(n_estimators=131,\n",
    "#                              max_depth=33,\n",
    "#                              n_jobs=-1,\n",
    "#                              max_features=0.445,\n",
    "#                              min_samples_leaf=1,\n",
    "#                              min_samples_split=number \n",
    "#                              )\n",
    "\n",
    "#     clf.fit(\n",
    "#         train_data.drop(['target'], axis=1),\n",
    "#         train_data['target']\n",
    "#     )\n",
    "    \n",
    "#     y_pred = clf.predict(X_val)\n",
    "    \n",
    "#     f1 = f1_score(y_val, y_pred)\n",
    "#     print(\"F1 score:\", f1,number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
